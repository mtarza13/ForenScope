from __future__ import annotations

import json
from dataclasses import dataclass
from pathlib import Path
from typing import Any

from .actions import record_action
from .case import ensure_case_layout, load_case
from .custody import append_entry
from .db import init_db
from .entropy import file_entropy
from .paths import resolve_item_path
from .store import fetch_items, insert_finding
from .strings_util import extract_strings
from .time_utils import now_utc_iso


@dataclass(frozen=True)
class EntropyResult:
    label: str | None
    created_at: str
    json_path: Path
    top: list[dict[str, Any]]


def run_entropy(
    *,
    case_path: Path,
    label: str | None,
    actor: str,
    top: int = 100,
    threshold: float = 7.2,
    max_files: int | None = None,
) -> EntropyResult:
    case_path = case_path.resolve()
    _ = load_case(case_path)
    ensure_case_layout(case_path)
    db_path = case_path / "db" / "dfirlab.sqlite"
    init_db(db_path)

    created_at = now_utc_iso()
    safe_ts = created_at.replace(":", "").replace("+", "").replace("-", "")

    rows = fetch_items(db_path, label=label)
    candidates = rows if max_files is None else rows[: int(max_files)]

    ent_rows: list[dict[str, Any]] = []
    for r in candidates:
        abs_path = resolve_item_path(case_path, r)
        if abs_path.is_symlink() or not abs_path.exists():
            continue
        ent = file_entropy(abs_path)
        ent_rows.append(
            {
                "item_id": int(r["item_id"]),
                "evidence_label": str(r["evidence_label"]),
                "rel_path": str(r["rel_path"]),
                "size": int(r["size"]),
                "entropy": float(ent),
                "sha256": r["sha256"],
            }
        )

    ent_rows.sort(key=lambda x: (-float(x["entropy"]), -int(x["size"]), int(x["item_id"])))
    top_rows = ent_rows[: max(0, int(top))]

    for row in top_rows:
        if float(row["entropy"]) >= float(threshold):
            insert_finding(
                db_path,
                item_id=int(row["item_id"]),
                kind="high_entropy",
                severity="medium",
                title=f"High entropy file (>= {threshold:.2f})",
                details=row,
                created_at=created_at,
            )

    out_dir = case_path / "artifacts" / "triage"
    out_dir.mkdir(parents=True, exist_ok=True)
    json_path = out_dir / f"entropy_{label or 'ALL'}_{safe_ts}.json"
    json_path.write_text(json.dumps({"schema_version": 1, "created_at": created_at, "label": label, "top": top_rows}, ensure_ascii=False, sort_keys=True, indent=2) + "\n", encoding="utf-8")

    append_entry(
        case_path / "chain_of_custody.log",
        actor=actor,
        action="entropy",
        details={"label": label, "top": int(top), "threshold": float(threshold), "report_path": str(json_path)},
    )
    record_action(
        db_path,
        timestamp=created_at,
        actor=actor,
        action="entropy",
        details={"label": label, "top": int(top), "threshold": float(threshold), "report_path": str(json_path)},
    )
    return EntropyResult(label=label, created_at=created_at, json_path=json_path, top=top_rows)


@dataclass(frozen=True)
class StringsCommandResult:
    label: str | None
    created_at: str
    json_path: Path
    files_processed: int


def run_strings(
    *,
    case_path: Path,
    label: str | None,
    actor: str,
    min_len: int = 6,
    max_bytes_per_file: int = 5 * 1024 * 1024,
    per_file_limit: int = 200,
    max_files: int = 50,
    item_ids: list[int] | None = None,
) -> StringsCommandResult:
    case_path = case_path.resolve()
    _ = load_case(case_path)
    ensure_case_layout(case_path)
    db_path = case_path / "db" / "dfirlab.sqlite"
    init_db(db_path)

    created_at = now_utc_iso()
    safe_ts = created_at.replace(":", "").replace("+", "").replace("-", "")

    rows = fetch_items(db_path, label=label)
    if item_ids:
        wanted = {int(x) for x in item_ids}
        rows = [r for r in rows if int(r["item_id"]) in wanted]

    results: list[dict[str, Any]] = []
    processed = 0
    for r in rows:
        if processed >= int(max_files):
            break
        abs_path = resolve_item_path(case_path, r)
        if abs_path.is_symlink() or not abs_path.exists():
            continue
        size = int(r["size"])
        if size > int(max_bytes_per_file):
            continue
        sr = extract_strings(abs_path, min_len=int(min_len), max_bytes=int(max_bytes_per_file), per_file_limit=int(per_file_limit))
        results.append(
            {
                "item_id": int(r["item_id"]),
                "evidence_label": str(r["evidence_label"]),
                "rel_path": str(r["rel_path"]),
                "size": size,
                "total_ascii": int(sr.total_ascii),
                "total_utf16le": int(sr.total_utf16le),
                "sample_ascii": sr.ascii_strings[:50],
                "sample_utf16le": sr.utf16le_strings[:50],
            }
        )
        processed += 1

    out_dir = case_path / "artifacts" / "triage"
    out_dir.mkdir(parents=True, exist_ok=True)
    json_path = out_dir / f"strings_{label or 'ALL'}_{safe_ts}.json"
    json_path.write_text(
        json.dumps(
            {
                "schema_version": 1,
                "created_at": created_at,
                "label": label,
                "min_len": int(min_len),
                "max_bytes_per_file": int(max_bytes_per_file),
                "per_file_limit": int(per_file_limit),
                "max_files": int(max_files),
                "results": results,
            },
            ensure_ascii=False,
            sort_keys=True,
            indent=2,
        )
        + "\n",
        encoding="utf-8",
    )

    append_entry(
        case_path / "chain_of_custody.log",
        actor=actor,
        action="strings",
        details={"label": label, "files_processed": processed, "report_path": str(json_path), "min_len": int(min_len)},
    )
    record_action(
        db_path,
        timestamp=created_at,
        actor=actor,
        action="strings",
        details={"label": label, "files_processed": processed, "report_path": str(json_path), "min_len": int(min_len)},
    )
    return StringsCommandResult(label=label, created_at=created_at, json_path=json_path, files_processed=processed)
